{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag_set'] = df['tags'].map(lambda s: set(s.split(' ')))\n",
    "\n",
    "tags = set()\n",
    "for t in df['tags']:\n",
    "    s = set(t.split(' '))\n",
    "    tags = tags | s\n",
    "\n",
    "tag_list = list(tags)\n",
    "tag_list.sort()\n",
    "tag_columns = ['tag_' + t for t in tag_list]\n",
    "for t in tag_list:\n",
    "    df['tag_' + t] = df['tag_set'].map(lambda x: 1 if t in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[tag_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[tag_columns].sum().sort_values().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tags_count = df.groupby('tags').count().sort_values(by='image_name', ascending=False)['image_name']\n",
    "print('There are {} unique tag combinations'.format(len(tags_count)))\n",
    "print()\n",
    "print(tags_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def display(images, cols=None, maxcols=10, width=16, titles=None):\n",
    "    if cols is None:\n",
    "        cols = len(images)\n",
    "    n_cols = cols if cols < maxcols else maxcols\n",
    "    plt.rc('axes', grid=False)\n",
    "    fig1 = plt.figure(1, (width, width * math.ceil(len(images)/n_cols)))\n",
    "    grid1 = ImageGrid(\n",
    "                fig1,\n",
    "                111,\n",
    "                nrows_ncols=(math.ceil(len(images)/n_cols), n_cols),\n",
    "                axes_pad=(0.1, 0.6)\n",
    "            )\n",
    "\n",
    "    for index, img in enumerate(images):\n",
    "        grid1[index].grid = False\n",
    "        if titles is not None:\n",
    "            grid1[index].set_title('\\n'.join(wrap(titles[index], width=25)))\n",
    "        if len(img.shape) == 2:\n",
    "            grid1[index].imshow(img, cmap='gray')\n",
    "        else:\n",
    "            grid1[index].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "def load_image(filename, resize=True, folder='train-jpg'):\n",
    "    img = io.imread('{}/{}.jpg'.format(folder, filename))\n",
    "    #print(img.shape)\n",
    "    #r, g, b, nir = img[:, :, 0], img[:, :, 1], img[:, :, 2], img[:, :, 3]\n",
    "    #print(nir.min(), nir.max())\n",
    "    if resize:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "    return np.array(img)\n",
    "\n",
    "def mean_normalize(img):\n",
    "    work = img.copy().astype(np.float32)\n",
    "    for c in range(work.shape[-1]):\n",
    "        w = work[...,c]\n",
    "        work[...,c] = (w - w.mean()) / (w.max() - w.min())\n",
    "    return work\n",
    "\n",
    "def normalize(img):\n",
    "    return img / 127.5 - 1\n",
    "\n",
    "def enhance(img):\n",
    "    work = img.copy()\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    for c in range(work.shape[-1]):\n",
    "        work[..., c] = clahe.apply(work[..., c])\n",
    "    return work\n",
    "\n",
    "def unnormalize(img):\n",
    "    work = img.copy()\n",
    "    for c in range(work.shape[-1]):\n",
    "        w = work[...,c]\n",
    "        work[...,c] = 255 * (w - w.min()) / (w.max() - w.min())\n",
    "    return work.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nir = sample_images[0][:,:,3]\n",
    "#nir.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df.sample(36)\n",
    "sample_images = [load_image(fn) for fn in samples['image_name']]\n",
    "INPUT_SHAPE = sample_images[0].shape\n",
    "print(INPUT_SHAPE)\n",
    "display(\n",
    "    sample_images,\n",
    "    cols=6,\n",
    "    titles=[t for t in samples['tags']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def my_preprocess(img):\n",
    "    img = mean_normalize(img)\n",
    "    return img\n",
    "\n",
    "preprocess = my_preprocess\n",
    "\n",
    "si = np.array(sample_images[:2])\n",
    "display(unnormalize(preprocess(si)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    [unnormalize(preprocess(img)) for img in sample_images],\n",
    "    cols=6,\n",
    "    titles=[t for t in samples['tags']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_train <- select up to 300 samples per tag\n",
    "samples_per_tag = 300\n",
    "df_train = []\n",
    "for t in tag_columns:\n",
    "    if t in ('tag_primary', 'clear'):\n",
    "        continue\n",
    "    tag_condition = df[t] == 1\n",
    "    take = min(samples_per_tag, tag_condition.sum())\n",
    "    df_train.append(df[tag_condition].sample(take).values)\n",
    "\n",
    "df_train = np.concatenate(df_train)\n",
    "df_train = pd.DataFrame(df_train)\n",
    "df_train.columns = df.columns\n",
    "\n",
    "df_train[tag_columns].sum()\n",
    "df_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['image_name'].values\n",
    "y = df_train[tag_columns].values\n",
    "\n",
    "n_features = 1\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print('We\\'ve got {} feature rows and {} labels'.format(len(X_train), len(y_train)))\n",
    "print('Each row has {} features'.format(n_features))\n",
    "print('and we have {} classes'.format(n_classes))\n",
    "assert(len(y_train) == len(X_train))\n",
    "print('We use {} rows for training and {} rows for validation'.format(len(X_train), len(X_valid)))\n",
    "print('Each image has the shape:', INPUT_SHAPE)\n",
    "print('So far, so good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Memory usage (train) kB', X_train.nbytes//(1024))\n",
    "print('Memory usage (valid) kB', X_valid.nbytes//(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fliplr(img, y):\n",
    "    return np.fliplr(img.copy()), y\n",
    "\n",
    "def flipud(img, y):\n",
    "    return np.flipud(img.copy()), y\n",
    "\n",
    "def adjust_brightness(img, y, amount):\n",
    "    result = img.astype(np.int16)\n",
    "    result += amount\n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    return result, y\n",
    "\n",
    "def _augment(X, y):\n",
    "    new_X, new_y = [], []\n",
    "    def add(nx, ny):\n",
    "        new_X.append(nx)\n",
    "        new_y.append(ny)\n",
    "\n",
    "    amount = int(random.uniform(5, 15))\n",
    "    add(X, y)\n",
    "    add(*fliplr(X, y))\n",
    "    add(*flipud(X, y))\n",
    "    add(*flipud(*fliplr(X, y)))\n",
    "    add(*adjust_brightness(*fliplr(X, y), amount))\n",
    "    add(*adjust_brightness(*flipud(X, y), -amount))\n",
    "    add(*adjust_brightness(*(X, y), amount))\n",
    "    add(*adjust_brightness(*(X, y), -amount))\n",
    "\n",
    "    return new_X, new_y\n",
    "\n",
    "def generator(X, y, augment=False, batch_size=32):\n",
    "    X_copy, y_copy = X, y\n",
    "    while True:\n",
    "        for i in range(0, len(X_copy), batch_size):\n",
    "            X_result, y_result = [], []\n",
    "            for x, y in zip(X_copy[i:i+batch_size], y_copy[i:i+batch_size]):\n",
    "                rx, ry = load_image(x), y\n",
    "\n",
    "                if augment:\n",
    "                    rx, ry = _augment(rx, ry)\n",
    "                else:\n",
    "                    rx, ry = [rx], [ry]\n",
    "\n",
    "                rx = np.array([preprocess(x) for x in rx])\n",
    "                ry = np.array(ry)\n",
    "                X_result.append(rx)\n",
    "                y_result.append(ry)\n",
    "            X_result, y_result = np.concatenate(X_result), np.concatenate(y_result)\n",
    "            yield shuffle(X_result, y_result)\n",
    "        X_copy, y_copy = shuffle(X_copy, y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "\n",
    "# ---------------------------------- #\n",
    "\n",
    "def my_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(48, (8, 8), strides=(2, 2), input_shape=INPUT_SHAPE, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (8, 8), strides=(2, 2), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(96, (3, 3), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(96, (3, 3), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(256, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def resnet_model():\n",
    "    base = ResNet50(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n",
    "    \n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = Flatten()(base.output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(256, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    preds = Dense(n_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    head = Model(inputs=base.input, outputs=preds)\n",
    "\n",
    "    return head\n",
    "    \n",
    "model = resnet_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[fbeta, 'accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"resnet-weights-improvement-{epoch:02d}-{val_fbeta:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_fbeta', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "BATCH = 16\n",
    "PER_EPOCH = 128\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_valid, y_valid = shuffle(X_valid, y_valid)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator(X_train, y_train, augment=False, batch_size=BATCH),\n",
    "    steps_per_epoch=PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator(X_valid, y_valid, batch_size=BATCH),\n",
    "    validation_steps=len(y_valid)//(2*4*BATCH),\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model.h5')\n",
    "model.load_weights('resnet-weights-improvement-02-0.869.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_test = os.listdir('test-jpg')\n",
    "X_test = [fn.replace('.jpg', '') for fn in X_test]\n",
    "\n",
    "result = []\n",
    "TEST_BATCH = 24\n",
    "for i in tqdm(range(0, len(X_test), TEST_BATCH)):\n",
    "    X_batch = X_test[i:i+TEST_BATCH]\n",
    "    X_batch = np.array([preprocess(load_image(fn, folder='test-jpg')) for fn in X_batch])\n",
    "    p = model.predict(X_batch)\n",
    "    result.append(p)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.concatenate(result)\n",
    "r = r > 0.5\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = []\n",
    "for row in r:\n",
    "    t = []\n",
    "    for b, v in zip(row, tag_columns):\n",
    "        if b:\n",
    "            t.append(v.replace('tag_', ''))\n",
    "    table.append(' '.join(t))\n",
    "#list(zip(X_test, table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame.from_dict({'image_name': X_test, 'tags': table})\n",
    "df_pred.to_csv('submission7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
